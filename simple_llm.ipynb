{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "505c8bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "import dotenv\n",
    "from typing import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40f3163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "model = GoogleGenerativeAI(\n",
    "    model= \"gemini-2.5-flash\",\n",
    "    temperature=0.7  # 0-> deterministic, 2->creative\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cdd4b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMState(TypedDict):\n",
    "    question: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a30ce370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_qa(state: LLMState):\n",
    "    ques = state[\"question\"]\n",
    "    prompt = f\"Answer the following question: {ques}\"\n",
    "    response = model.invoke(prompt)\n",
    "    \n",
    "    return {\"answer\": response}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "510b08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(LLMState)\n",
    "\n",
    "\n",
    "graph.add_node('chatmodel',llm_qa)\n",
    "\n",
    "\n",
    "graph.add_edge(START, 'chatmodel')\n",
    "graph.add_edge('chatmodel', END)\n",
    "\n",
    "workflow = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b663f5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What is the KNN?', 'answer': '**K-Nearest Neighbors (KNN)** is a simple, non-parametric, supervised machine learning algorithm that can be used for both **classification** and **regression** tasks. It\\'s considered a \"lazy learner\" because it doesn\\'t explicitly build a model during the training phase; instead, it memorizes the entire training dataset. The computation happens only when a prediction is requested.\\n\\n## How K-Nearest Neighbors Works:\\n\\nThe core idea behind KNN is to find the **\\'k\\' closest data points** (neighbors) in the training data to a new, unseen data point, and then use the information from these neighbors to make a prediction.\\n\\nLet\\'s break down the process:\\n\\n1.  **Store Training Data:** During the \"training\" phase, the KNN algorithm simply stores all the labeled training data points (features and their corresponding class labels or values).\\n\\n2.  **Receive New Data Point:** When a new, unlabeled data point arrives for which we want to make a prediction:\\n\\n3.  **Calculate Distance:** The algorithm calculates the distance between this new data point and *every single* data point in the training set. Common distance metrics include:\\n    *   **Euclidean Distance:** The most common, representing the straight-line distance between two points in Euclidean space.\\n    *   **Manhattan Distance:** The sum of the absolute differences of their Cartesian coordinates (like navigating a city grid).\\n    *   **Minkowski Distance:** A generalization of Euclidean and Manhattan distances.\\n\\n4.  **Identify \\'k\\' Nearest Neighbors:** After calculating all distances, the algorithm identifies the `k` data points from the training set that are closest (have the smallest distances) to the new data point. The value of `k` is a user-defined hyperparameter.\\n\\n5.  **Make a Prediction:**\\n    *   **For Classification:** The new data point is assigned the class label that is most frequent among its `k` nearest neighbors (a majority vote). For example, if `k=5` and 3 neighbors are \"Class A\" and 2 are \"Class B\", the new point is classified as \"Class A\".\\n    *   **For Regression:** The predicted value for the new data point is typically the average (mean) or median of the target values of its `k` nearest neighbors.\\n\\n## Key Characteristics:\\n\\n*   **Non-parametric:** It makes no assumptions about the underlying distribution of the data.\\n*   **Lazy Learner:** It doesn\\'t learn a discriminative function from the training data but rather \"memorizes\" the training dataset. All computation is deferred until prediction.\\n*   **Instance-Based Learning:** It directly uses instances from the training data to make predictions.\\n*   **Hyperparameter `k`:** The choice of `k` is crucial.\\n    *   A small `k` (e.g., `k=1`) makes the model highly sensitive to noise and outliers, leading to high variance.\\n    *   A large `k` makes the model smoother and less sensitive to noise but can blur class boundaries and potentially introduce bias.\\n    *   `k` is often chosen via cross-validation. For classification, an odd `k` is often preferred to avoid ties.\\n\\n## Advantages:\\n\\n*   **Simple and Easy to Understand:** The logic is intuitive.\\n*   **No Training Phase:** As a lazy learner, there\\'s no explicit training time.\\n*   **Flexible:** Can be used for both classification and regression.\\n*   **Adapts to Complex Decision Boundaries:** Can model non-linear relationships.\\n*   **Works for Multi-Class Problems:** Easily extends to more than two classes.\\n\\n## Disadvantages:\\n\\n*   **Computationally Expensive at Prediction Time:** It needs to calculate distances to *all* training points for each new prediction, which can be slow for large datasets.\\n*   **Memory Intensive:** It needs to store the entire training dataset.\\n*   **Sensitive to Irrelevant Features:** All features contribute to the distance calculation, so irrelevant features can degrade performance. Feature scaling is often necessary.\\n*   **Curse of Dimensionality:** Performance degrades significantly in very high-dimensional feature spaces, as distances become less meaningful.\\n*   **Sensitive to Imbalanced Data:** If one class is heavily represented, it can dominate the majority vote among neighbors, leading to biased predictions.\\n\\n## When to Use KNN:\\n\\nKNN is a good choice for small to medium-sized datasets, especially when the decision boundary is complex and not easily modeled by linear methods. It\\'s often used as a baseline model due to its simplicity. However, for very large datasets or high-dimensional data, more efficient algorithms are usually preferred.'}\n"
     ]
    }
   ],
   "source": [
    "initial_state: LLMState = {\n",
    "    \"question\": \"What is the KNN?\"\n",
    "}\n",
    "\n",
    "final_state = workflow.invoke(initial_state)\n",
    "print(final_state)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
